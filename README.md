# üìÑ Chatbot over PDFs using Retrieval-Augmented Generation (RAG)

## üîç Overview
This project implements a **Retrieval-Augmented Generation (RAG)** based chatbot that allows users to upload PDF documents and ask questions in natural language.

The chatbot generates answers **strictly grounded in the content of the uploaded PDFs**, reducing hallucinations commonly observed in standalone language models.

---

## üß† Key Idea
Instead of training a language model on PDFs, the system follows a retrieval-based approach:

- Extract and clean text from PDFs  
- Split text into overlapping chunks  
- Convert chunks into dense vector embeddings  
- Store embeddings in a vector database  
- Retrieve relevant chunks at query time  
- Generate answers using an LLM **only from retrieved context**

> **Important:**  
> The language model itself is **not trained on PDFs**.

---

## üèóÔ∏è Architecture
```bash


                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ    PDFs    ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚Üì
              Text Extraction / OCR
                      ‚Üì
                 Text Chunking
                      ‚Üì
              Embedding Generation
                      ‚Üì
              Vector Database
                      ‚Üë
User Query ‚Üí Embedding ‚Üí Retrieval
                      ‚Üì
                  LLM Prompt
                      ‚Üì
                   Answer
```

---

## üõ†Ô∏è Tech Stack

- **Language:** Python  
- **UI:** Streamlit  
- **Embeddings:** Sentence Transformers  
- **Vector Store:** FAISS  
- **LLM (Local Development):** Ollama (Mistral)

> Ollama is used during local development to avoid API costs and quota limits.  
> The LLM backend can be replaced with a hosted model (e.g., OpenAI) during deployment.

---

## üöÄ How to Run Locally

### 1Ô∏è‚É£ Create a virtual environment
```bash
python -m venv venv
source venv/bin/activate
```
### 2Ô∏è‚É£ Install dependencies
```bash
pip install -r requirements.txt
```
### 3Ô∏è‚É£ Run the application
```bash
streamlit run app.py
```

## ‚úÖ Features

- Upload and process PDF documents
- Semantic search using dense embeddings
- Context-aware question answering using RAG
- Local LLM inference (no API keys required)
- Debug view showing retrieved context

## ‚ö†Ô∏è Limitations

- OCR is not enabled for scanned PDFs
- Local LLM responses may be less fluent than cloud-based models
- Designed for learning, interviews, and demos rather than production scale

## üìå Future Improvements

- Add citations with page numbers
- Enable OCR for scanned PDFs
- Add configurable LLM backend switching (Ollama ‚Üî OpenAI)
- Deploy the application on Streamlit Cloud
