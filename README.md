# ğŸ“„ Chatbot over PDFs using Retrieval-Augmented Generation (RAG)

## ğŸ” Overview
This project implements a **Retrieval-Augmented Generation (RAG)** based chatbot that allows users to upload PDF documents and ask questions in natural language.

The chatbot generates answers **strictly grounded in the content of the uploaded PDFs**, reducing hallucinations commonly observed in standalone language models.

---

## ğŸ§  Key Idea
Instead of training a language model on PDFs, the system follows a retrieval-based approach:

- Extract and clean text from PDFs  
- Split text into overlapping chunks  
- Convert chunks into dense vector embeddings  
- Store embeddings in a vector database  
- Retrieve relevant chunks at query time  
- Generate answers using an LLM **only from retrieved context**

> **Important:**  
> The language model itself is **not trained on PDFs**.

---

## ğŸ—ï¸ Architecture

PDF â†’ Text Extraction â†’ Chunking â†’ Embeddings â†’ Vector Database
â†‘
User Query â†’ Embedding â†’ Retrieval â†’ Context â†’ LLM â†’ Answer


---

## ğŸ› ï¸ Tech Stack

- **Language:** Python  
- **UI:** Streamlit  
- **Embeddings:** Sentence Transformers  
- **Vector Store:** FAISS  
- **LLM (Local Development):** Ollama (Mistral)

> Ollama is used during local development to avoid API costs and quota limits.  
> The LLM backend can be replaced with a hosted model (e.g., OpenAI) during deployment.

---

## ğŸš€ How to Run Locally

### 1ï¸âƒ£ Create a virtual environment
```bash
python -m venv venv
source venv/bin/activate

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt